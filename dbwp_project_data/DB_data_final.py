# -*- coding: utf-8 -*-
"""DB_realFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H63pyiqZSN8ai4oLgcZZY2759L4SK-8g
"""

import snscrape.modules.twitter as sntwitter
import datetime
import json
import pandas as pd
import boto3
from collections import Counter

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
from textblob import TextBlob

def preprocessing(lines):
    words = lines.select(explode(split("0", "t_end")).alias("word"))
    words = words.na.replace('', None)
    words = words.na.drop()
    words = words.withColumn('word', F.regexp_replace('word', r'http\S+', ''))
    words = words.withColumn('word', F.regexp_replace('word', '@\w+', ''))
    words = words.withColumn('word', F.regexp_replace('word', '#', ''))
    words = words.withColumn('word', F.regexp_replace('word', 'RT', ''))
    words = words.withColumn('word', F.regexp_replace('word', ':', ''))
    return words

# text classification
def polarity_detection(text):
    return TextBlob(text).sentiment.polarity
def subjectivity_detection(text):
    return TextBlob(text).sentiment.subjectivity
def text_classification(words):
    # polarity detection
    polarity_detection_udf = udf(polarity_detection, StringType())
    words = words.withColumn("polarity", polarity_detection_udf("word"))
    # subjectivity detection
    subjectivity_detection_udf = udf(subjectivity_detection, StringType())
    words = words.withColumn("subjectivity", subjectivity_detection_udf("word"))
    return words

def tweetssave(trend):
    tmp = []
    for i, tweet in enumerate(sntwitter.TwitterSearchScraper
    (f'{trend} since:{"before_one_day"} until:{"now"}').get_items()):
        tmp.append([tweet.content])
    tweets.append(tmp)
    del tmp

session = boto3.Session( 
         aws_access_key_id='AKIASTN5LVL4YNKT7ERV', 
         aws_secret_access_key='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')

#session to get the resource
s3 = session.resource('s3')

bucket = s3.Bucket('dbwpteam1')

resultTrend = []

for obj in bucket.objects.all():
  content_object = s3.Object('dbwpteam1', obj.key)
  file_content = content_object.get()['Body'].read().decode('utf-8')
  json_content = json.loads(file_content)
  resultTrend.extend(json_content)

uniqueTrendAndCounting = Counter(resultTrend)
sorted_data = sorted(uniqueTrendAndCounting.items(), key = lambda item: item[1], reverse = True)

with open('unique_trend.json', 'w', encoding="utf-8") as make_file:
  json.dump(sorted_data, make_file, ensure_ascii=False, indent="\t")

trends = []
for i in range(len(sorted_data)):
  trends.append(sorted_data[i][0])

# create Spark session
spark = SparkSession.builder.appName("TwitterSentimentAnalysis").getOrCreate()

resultPolarity = {}
resultSubjectivity = {}

for trend in trends:
  tweets = []
  tweetssave(trend)
  df_test = pd.DataFrame(tweets[0])
  df_spark = spark.createDataFrame(df_test)
  words = preprocessing(df_spark)

  # text classification to define polarity and subjectivity
  words = text_classification(words)

  words = words.repartition(1)

  sum = 0
  for i in range(words.count()):
    sum += float(words.collect()[i][1])
  mean = sum/words.count()
  resultPolarity[trend] = mean

  sum2 = 0
  for i in range(words.count()):
    sum2 += float(words.collect()[i][2])
  mean = sum2/words.count()
  resultSubjectivity[trend] = mean

with open('resultPolarity.json', 'w', encoding="utf-8") as make_file:
  json.dump(resultPolarity, make_file, ensure_ascii=False, indent="\t")

with open('resultSubjectivity.json', 'w', encoding="utf-8") as make_file:
  json.dump(resultSubjectivity, make_file, ensure_ascii=False, indent="\t")